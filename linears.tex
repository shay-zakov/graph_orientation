\section{A generic algorithm for linear graphs}\label{s.2}
We call a bi-weighted graph with $n$ edges and $n+1$ vertices a\emph{ linear graph}, and 
reserve the term path for a directed linear graph that has a single source.
Number the vertices of the linear graph  $L_n$ from \textit{0} to \textit{n}, and denote the weights of
edges $(i,i+1)$ and  $(i+1,i)$ by $w(i,i+1)$ and $w(i+1,1)$, respectively.

We describe here an algorithm for finding an optimal orientation 
of a linear graph. The high level version of the algorithm makes no use of the details
of the cost function, be it $H_m$ or $H_s$. The description therefore uses the subscript $x$ with $x\in \{s,m\}$. However, upon implementing the high level version, the difference between 
the two cost functions leads to a surprising difference in running times.
\bigskip

{\bf Notation}:
\begin{itemize}
\item $L_{i, j}$ is the sub-graph of $L_n$ induced by the vertices $i,  \ldots, j$. 
\item $\rdir{L}_{i, j}$ denotes the oriented version 
of $L_{i, j}$ in which all edges are directed to the right (i.e. of the form $(i, {i+1})$),
and $\ldir{L}_{i, j}$ denotes the oriented version 
of $L_{i, j}$ in which all edges are directed to the left. 
\item $\rlast{H}_x(i)$ is the value of an optimal orientaton of $L_{0, i}$ under the constraint
that edge $\{i-1,i\}$ is directed towards $i$, and $H_x^{\prec}(i)$ is the value of an optimal orientaton of $L_{0, i}$ under the constraint
that edge $\{i-1,i\}$ is directed towards $i-1$.\\
In particular, the cost of an optimal orientation of $L_n$ under $h_x$ is 
$\min \{H_x^{\succ}(n),\ H_x^{\prec}(n)\}$.
\end{itemize}

The basic step of the recursive algorithm is to locate the last 
vertex of an optimal orientation of $L_{0, j}$ at which there is a change in direction,
given that the very last edge has a specified direction. 
\bigskip

\begin{algorithm}
%	Allocate two null arrays $H^{\succ}$ and $H^{\prec}$ of length $n=length(L)+1$\;
	$H^{\succ}(0)=H^{\prec}(0)=-\infty$, $n=length(L)+1$\;
	\For{$j=1$ to $n$}{
		set $H^{\succ}(j)$ to the minimum over $0\leq i <j$ of $\max \{ H^{\prec}(i), h_x(\vec{L}_{i, j})\}$\;
		\label{st.1}
		set $H^{\prec}(j)$ to the minimum over $0\leq i <j$ of  $\max \{ H^{\succ}(i), h_x(\cev{L}_{i, j})\}$\;
			\label{st.2}
	}
	\Return{$\min \{H^{\succ}(n),\ H^{\prec}(n)\}$}\;
	\caption{BestCostPath-$x$ $(L)$}
	\label{algo:H}
\end{algorithm}
\begin{theorem}
Given input $L$, Algorithm \emph{BestCostPath}-$x$ finds the cost of an optimal orientation of $L$.
\end{theorem}
A straightforward implementation of the algorithm runs in $O(n^2)$ time. 
The implementations described in the next two subsections provide drastic improvements.

\subsection{Running time under cost function $H_s$}
In this subsection we show that 
Algorithm BestCostPath-s can be made to run in time $O(n)$ by carefully implementing
the computational operations of the algorithm.
Two issues have to be addressed.

	The first is the time needed to compute $h_s(\vec{L}_{i, j})$ 
	and $h_s(\cev{L}_{i, j})$ for given $0\leq i<j\leq n$.
	Focusing on $h_s(\vec{L}_{i, j})$, and combining equations (\ref{eq.W}) and (\ref{eq.hs}):
	\begin{equation}\label{eq.hsij}
	h_s(\vec{L}_{i, j})=\max \{ \sum_{t=i'}^{j'-1}w(t,t+1) \mid i\leq i' \leq j' \leq j\}.
	\end{equation}
	
	Computing $h_s(\vec{L}_{i, j})$ for a pair $(i,j)$ is therefore an instance
	of the Range Maximum-sum Segment On-line Query problem, 
	RMSOQ for short, as defined in \cite{chen2007range}:
	\begin{problem}[Range Maximum-sum Segment On-line Query problem]\ \\
		\noindent \emph{\bf Input to be preprocessed:}
		A nonempty sequence $a_1 ,\ldots a_n$ of real numbers.\\ 
		\noindent \emph{\bf Online query:} respond to a query of the form $RMSOQ( i, j)$ by returning a pair of indices $(i', j')$ 
		which maximize
		$\sum_{t=i'}^{j'}a_t$ over all $i\leq i' \leq j' \leq j$.	
	\end{problem}
	Chen and Chao  \cite{chen2007range} presented a method for answering each such query 
	in constant time after  $A$ is preprocessed in $O(n)$ time. The following Lemma 
	summarizes the discussion and its relevance.
\begin{lemma}
	Suppose $w(i,i+1),0\leq i <n$ and $w(i+1,i),0\leq i <n$ have been preprocessed 
	in linear time for the RMSOQ problem.
	After  $H^{\prec}(i)$ and $H^{\succ}(i)$ have been computed for $0\leq i <j$ all 
	values $\max \{ H^{\prec}(i), h_s(\vec{L}_{i, j})\}$ and $\max \{ H^{\succ}(i), h_s(\cev{L}_{i, j})\}$ appearing in statements \ref{st.1} and \ref{st.2} in iteration $j$
	can be evaluated in constant time.
\end{lemma}
A question remains: how many queries $RMSOQ( i, j)$ will the algorithm present?
The answer to this question will be found by addressing the second issue: what is the time needed to find all minimum values in statements \ref{st.1} and \ref{st.2}. Here the notion of a totally monotone matrix will be helpful. 
To emphasize that this monotinicity is columnwise we call it c-monitinicity.
\begin{definition}\label{d.tm}
	Given an $n \times n$ matrix $M$, denote
	by $R(j)$ the largest row index at which the minimum in column $j$ of $M$ is achieved, 
	i.e.,
	$R(j) = \max \{k :M_{k,j} = min_{1\leq i \leq n} M_{i,j}\}$.
	The matrix $M$ is c-monotone if $R(1) \leq  R(2)\leq \cdots \leq R(n)$, and it is totally c-monotone 
	if all $2\times 2$
	submatrices of $M$ formed by choosing two rows and two columns are c-monotone.
\end{definition}
\begin{lemma}\label{l.cmono}
	$M$ is totally c-monotone if and only if $(i_1,j_2)\geq M(i_2,j_2)$
	whenever $M(i_1,j_1)\geq M(i_2,j_1)$,
for all $i_1<i_2$ and $j_1<j_2$.
\end{lemma}
Whereas for an arbitrary square matrix it takes quadratic time to 
compute all values $R(j)$, for a c-monotone matrix the SMAWK algorithm \cite{smawk1987} 
is able to do so, off-line, in linear time.
	
Define the matrices $M^{\succ}(i,j)$ and $M^{\prec}(i,j)$ as follows.
\begin{itemize}
	\item For $0\leq i<j \leq n$,
$$M^{\succ}(i,j)=\max \{ H^{\prec}(i), h_s(\vec{L}_{i, j})\},\ 
M^{\prec}(i,j)=\max \{ H^{\succ}(i), h_s(\cev{L}_{i, j})\}.$$	
	\item $M^{\succ}(i,j)=M^{\prec}(i,j)=\infty$  for $1\leq j \leq i \leq n $.
%	\item $M^{\succ}(0,j)=h_s(\vec{L}_{0, j})$, $M^{\prec}(0,j)=h_s(\cev{L}_{0, j})$, $1\leq j\leq n$.
\end{itemize}
In these terms the algorithm computes the minimum value in column $j$ of 
the matrices $M^{\succ}$
and $M^{\prec}$, for $1\leq j \leq n$.
Two features of this computation deserve particular attention.
\begin{enumerate}
	\item The computation of $M^{\succ}$
	and $M^{\prec}$ has has an on-line flavor: before computing the minimum value in column $j$ of 
	the matrix $M^{\succ}$, or $M^{\prec}$, the minimum values of all columns $j'<j$ of 
	the matrix $M^{\prec}$, respectively $M^{\succ}$,  have to be available.
		\item \label{i.1} It follows from equation (\ref{eq.hsij}) that
	$h_s(\vec{L}_{i, j})$ and $h_s(\cev{L}_{i, j})$ are both non-increasing in $i$ 
	and non-decreasing in $j$. 
\end{enumerate}
According to the first feature, the problem of computing 
$H^{\succ}(j)$ and $H^{\succ}(j)$ can be dealt with by any method that solves the 
 following problem
\begin{problem}[OCM - Online Column Minima]
	For $1\leq j \leq n$ compute $H(j)=\min \{M(i,j) \mid 0\leq i \leq n\}$, where 
	the values of $H(j'),\ 1\leq j'<j$ have to be computed before $M(i,j)$ can be evaluated.
\end{problem}
In our case $M$ is of the form $M(i,j)=\max \{f(i),g(i,j)\}$.
The second feature, that $g$ is non-increasing in $i$ and non-decreasing in $j$, is the key to proving that our $M$ is in fact totally c-monotone.
\begin{proposition}
	Our $M$ is totally c-monotone.
\end{proposition}
\begin{proof}
	According to Lemma \ref{l.cmono} we have to prove that for any $i_1<i_2$ and $j_1<j_2$,
	 if $M(i_1,j_1)\geq M(i_2,j_1)$ then $M(i_1,j_2)\geq M(i_2,j_2)$.

For any $j$ and  $i_1<i_2$, $M(i_1,j)\geq M(i_2,j)$, i.e.
$\max \{ f(i_1), g(i_1, j)\}\geq \max \{ f(i_2), g(i_2, j)\}$,
if and only if 
$\max \{ f(i_1), g(i_1, j)\}\geq \ f(i_2)$, since $g(i_1, j)\geq g(i_2, j)$.

Consequently,  if $M(i_1,j)\geq M(i_2,j)$ then
$$ f(i_2)\leq \max \{ f(i_1), g(i_1, j_1)\}\leq \max \{ f(i_1), g(i_1, j_2)\},$$
since $g(i_1, j_1)\leq g(i_1, j_2)$.
Thus $M(i_1,j_2)\geq M(i_2,j_2)$.
\end{proof}
Summarizing the foregoing discussion, to find all minimum values in statements \ref{st.1} and \ref{st.2} of the algorithm we can employ the solution to the following problem.
\begin{problem}[OCMM - Online Column Minima of c-Monotone matrix]
		For $1\leq j \leq n$ compute $H(j)=\min \{M(i,j) \mid 0\leq i \leq n\}$, where 
		$M$ is c-monotone and
	the values of $H(j'),\ 1\leq j'<j$ have to be computed before $M(i,j)$ can be evaluated.
\end{problem}
Note that the SMAWK algorithm cannot
be used directly because it is off-line whereas our computation has to be online.
Fortunately, several linear-time online algorithms have been published, 
\cite{klawe89,larmore91,galil92,barnoy09}.
%\cite{klawe89},  \cite{larmore91}, \cite{galil92}, \cite{barnoy09}.

\begin{theorem}
	When the cost function used in Algorithm BestCostPath-x is $H_s$ its running time is $O(n)$.
\end{theorem}

\
