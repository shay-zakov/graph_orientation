
\section{An Algorithm for Linear Graphs}

%A \emph{linear graph} $P$ of length $n$ is an undirected graph with $n+1$
%vertices $V = \{v_0, v_1, \ldots, v_n\}$ and $n$ edges $E = \left\{\{v_i, v_{i+1}\} : 0\leq i < n\right\}$. 
Given a bi-weighted linear graph $P$ of length $n$ we assume that its vertices
are numbered from \textit{0} to \textit{n}, and denote the weights of
edges $(i,i+1)$ and  $(i+1,i)$ by $w(i,i+1)$ and $w(i+1,1)$, respectively.

We describe next an algorithm for finding an optimal orientation 
of a linear graph. The high level version of the algorithm makes no use of the details
of the cost function, be it $h_m$ or $h_s$. The description will therefore omit these subscripts. However, upon implementing the high level version, the difference between 
the two cost functions leads to a surprising difference in running times: it is linear 
under $h_s$ but quadratic under $h_m$.
\bigskip

{\bf Notation}:
\begin{itemize}
 	\item $P_{i, j}$ is the sub-graph of $P$ induced by the vertices $i,  \ldots, j$, and $P=P_{0,n}$. 
 	\item $\vec{P}_{i, j}$ denotes the oriented version 
 	of $P_{i, j}$ in which all edges are directed to the right (i.e. of the form $(k, {k+1})$),
 	and similarly  $\cev{P}_{i, j}$ denotes the oriented version 
 	of $P_{i, j}$ in which all edges are directed to the left.
 	\item $H_x(j)$ is the value of an optimal orientaton of $P_{0, j}$ with respect to the $h_x$ cost function, where $x = s$ or $x = m$. $H^r_x(j)$ and $H^{\ell}_x(j)$ are defined similarly, under the constraint 
 	that edge $\{j-1, j\}$ is directed either towards $j$ or towards $j-1$, respectively. Using this notation we are assuming that the original instance graph $P$ is clear from the context.
\end{itemize}

Clearly,

\begin{equation}
\label{eq:Hx}
H_x(j) = \min \left(H^r_x(j), H^\ell_x(j)\right).
\end{equation}

Consider next the set of orientations of $P_{0, j}$ in which the orientation of $\{j-1, j\}$ is directed towards $j$. In any such an orientation, there exists some unique $0 \leq i < j$ such that for every $i \leq k < j$ the edges $\{k, k+1\}$ are oriented as $(k, k+1)$, and either $i = 0$ or the edge $\{i-1, i\}$ is oriented as $(i, i-1)$. Any path in such an orientation is fully contained in either the induced orientation of the prefix $P_{0, i}$, or the induced orientation of the suffix $P_{i, j}$ (as both edges $(i, i-1)$ and $(i, i+1)$ cannot participate in a single path). In the former case, it is clear that an optimal orientation for the prefix would be one that minimizes the heaviest path weight for the edge bi-weighted subgraph $P_{0, i}$ under the constrain that $\{i-1, i\}$ is oriented as $(i, i-1)$. By definition, the weight of such an orientation is $H^\ell_x(i)$. For the suffix $P_{i, j}$ the orientation is fixed to $\vec{P}_{i, j}$, and the weight of a heaviest path in it is $h_x(\vec{P}_{i, j})$. Therefore, for a particular split index $i$ the minimum weight of an orientation as describe above would be $\min\left(H^\ell_x(i), h_x(\vec{P}_{i, j})\right)$, and considering all possible split indices we get the following:

\begin{equation}\label{eq:Hr}
H^r_x(j) = \left\{\begin{array}{ll}
0, & j = 0\\
\min_{0 \leq i < j}\left(\max\left(H^\ell_x(i), h_x(\vec{P}_{i, j})\right)\right), & \text{otherwise}\\
\end{array}\right.
\end{equation}

Symmetrically: 

\begin{equation}\label{eq:Hl}
H^\ell_x(j) = \left\{\begin{array}{ll}
0, & j = 0\\
\min_{0 \leq i < j}\left(\max\left(H^r_x(i), h_x(\cev{P}_{i, j})\right)\right), & \text{otherwise}\\
\end{array}\right.
\end{equation}

The recursive computation described by Equations~\ref{eq:Hx} to~\ref{eq:Hl} can be efficiently implemented in a bottom-up fashion. Algorithm~\ref{algo:H} describes this implementation, utilizing two arrays $H^r[\cdot]$ and $H^\ell[\cdot]$ to store values of the forms $H^r_x(i)$ and $H^\ell_x(i)$ respectively (we use rectangle brackets to distinguish the algorithm entity from the formal one to which it corresponds). For now, we keep abstract the manner in which values of the form $h_x(\vec{P}_{i, j})$ and $h_x(\cev{P}_{i, j})$ are being computed in the algorithm. Though it is not difficult to show all such values can be computed in quadratic running time and match the algorithm's running time (as explained below), we later show how can all components of the algorithm be computed more efficiently.

\begin{algorithm}
	Allocate two $n+1$-length arrays $H^r$ and $H^\ell$, and initialize both $H^r[0]$ and $H^{\ell}[0]$ to $0$\;
	\For{$j=1$ to $n$}{
		$H^{r}[j] \gets \min_{0\leq i <j} \max \{ H^{\ell}[i], h_x(\vec{P}_{i, j})\}$\;
		$H^{\ell}[j] \gets \min_{0\leq i <j} \max \{ H^r[i], h_x(\cev{P}_{i, j})\}$\;
	}
	\Return{$\min \{H^{r}[n],\ H^{\ell}[n]\}$}\;
	\caption{BestCostPath-$x$ $(P)$}
	\label{algo:H}
\end{algorithm}

\paragraph{Time and space complxities of Algorithm~\ref{algo:H}.}
This analysis ignores the computational resources needed for computing values of the form $h_x(\vec{P}_{i, j})$ and $h_x(\cev{P}_{i, j})$, which will be discussed later.
The time complexity of the algorithm is dictated by the loop in lines~2 to~4. This loop is executed $n$ times. The expressions computed within each loop iteration take $O(n)$ time to compute, as $O(n)$ split-points $i$ are examine in each iteration, and all values of the form $H^{\ell}[i]$ and $H^{r}[i]$ are pre-computed. Therefore, the overall time complexity of the algorithm is $O(n^2)$. The space complexity is $O(n)$ due to the need to maintain the $H$ arrays.


****************************************


Turning now to the running time analysis, we note first of all that the values 
$ h(\vec{P_{i, j}}), h(\cev{P_{i, j}})$ used in statements (\ref{i.1}) and (\ref{i.2})
can be computed in constant time if the cost function is $h_m$. Namely, if the $2n$ values 
$ h_m(\vec{P_{0, j}})$, $h_m(\cev{P_{0, j}})$, $1\leq j \leq n$ are precomputed in $O(n)$ time,
then
$h_m(\vec{P_{i, j}})=h_m(\vec{P_{0, j}})-h_m(\vec{P_{0, i}})
%=\sum_{k=i}^{j-1} w(k,k+1).$
$.

To prove that this is true also for the cost function $h_s$ takes more doing.

%It remains to determine the time needed for the computation of the minimum values 
%in those statements.
%On the face of it, it takes $O(n)$ time to compute one such value.
%We show now that the totality of these computations takes $O(n)$ time
%if the cost function is $h_s$.
%
%\begin{theorem}
%When the cost function used in Algorithm BestCostPath ($P$) is $h_s$ its running time is $O(n)$.
%\end{theorem}
%
%\noindent {\bf Proof}.
%According to the foregoing analysis we need to show that the total time needed for the computations of all minimum values in statements (\ref{i.1}) and (\ref{i.2}) is $O(n)$.
%The key to the proof is the following fact.
%\begin{claim}
%	Denote 
%	$$a^{\ell}[j]=\argmin_{0\leq i <j} \max \{ H_s^{\ell}[i], h_s(\vec{P_{i, j}}) \},
%   a^{r}[j]=\argmin_{0\leq i <j} \max \{ H_s^{r}[i], h_s(\vec{P_{i, j}}) \}.
%	$$
%	Then $a^{\ell}[j]\leq a^{\ell}[j+1]$, and $a^{r}[j]\leq a^{r}[j+1]$, $0\leq j <n$.
%\end{claim}
%
%\noindent The proof of the Claim rests on two observations:
%\begin{enumerate}
%	\item $h_s(\vec{P_{i, j}}$ is non-increasing as a function of $i$ and non-decreasing as a function of $j$. 
%\end{enumerate}
%\qed
%
%-----------------------------------------------------------------------------------------
%
%Let $P$ be a bi-weighted linear graph of length $n$. Denote:
%\begin{itemize}
%	\item $P_{i, j}$: the sub-graph of $P$ induced by the vertices $\left\{v_i, v_{i+1}, \ldots, v_j \right\}$. For short, denote by $P_j$ the prefix $P_{0, j}$ of $P$;
%	\item $\vec{P'} \vec{P''}$: an orientation of $P$ obtained by concatenating the orientations $\dir[P']$ and $\dir[P'']$ for some prefix $P_{k}$ and suffix $P_{k, n}$ of $P$, respectively;
%	\item $\dir[P^r]$, $\dir[P^l]$: the orientations of $P$ in which all edges are directed "rightward" (i.e. of the form $(v_i, v_{i+1})$) or "leftwards" (i.e. of the form $(v_{i+1}, v_i)$), respectively;
%	\item $\mathcal{O}^r(P) = \left\{\dir[P'] \dir[P^r]_{n-1,n} : \dir[P'] \in \mathcal{O}(P_{n-1})\right\}$ (i.e. the subset of $\mathcal{O}(P)$ containing all orientations of $P$ ending with the directed edge $(v_{n-1}, v_n)$);
%	\item $\mathcal{O}^l(P) = \mathcal{O}(P) \setminus \mathcal{O}^r(P) = \left\{\dir[P'] \dir[P^l]_{n-1,n} : \dir[P'] \in \mathcal{O}(P_{n-1})\right\}$ (i.e. the subset of $\mathcal{O}(P)$ containing all orientations of $P$ ending with the directed edge $(v_n, v_{n-1})$);
%	\item $H^r(P) = \min_{\dir[P] \in \mathcal{O}^r(P)} h(\dir[P])$;
%	\item $H^l(P) = \min_{\dir[P] \in \mathcal{O}^l(P)} h(\dir[P])$.
%\end{itemize}
%
%Before describing the computation of $H(P)$, we first describe an auxiliary computation of values of the form $h(\dir[P^r]_{i, j})$ which will be used within the computation of $H(P)$. This computation utilizes the \emph{deque} data structure to store information about $\dir[P^r]_{i, j}$ that will allow the efficient computation of $h(\dir[P^r]_{i, j})$. In addition, given the structure for the segment $\dir[P^r]_{i, j}$, the updated structure with respect to segments $\dir[P^r]_{i+1, j}$ and $\dir[P^r]_{i, j+1}$ can be computed efficiently. The computation of values of the form $h(\dir[P^l]_{i, j})$ is symmetric and we omit its explicit description. 
%
%Let $S = \langle s_0, s_1, ..., s_{m-1}\rangle$ be a series of numbers. When $m > 0$, denote by $k_S$ the index $0 \leq k < m$ so that $s_{k_S}$ is the maximum of $S$, and $k_S$ is the biggest such index if there are multiple maximal values in $S$. In other words, for every $k_S < k < m$, $s_{k_S} > s_k$. The \emph{decreasing maximum indices} $S^{\downarrow}$ of $S$ is inductively defined as follows. When $S$ is empty then $S^{\downarrow}$ is empty, and otherwise $S^{\downarrow}$ is the series of indices starting with $k_S$ and continuing with the decreasing maximum indices of the suffix $\langle s_{k_S+1}, s_{k_S+2}, ..., s_{m-1}\rangle$ of $S$. For example, for $S = \langle 5, 4, 5, 2, 3, -1, 0, 1\rangle$, $S^\downarrow = \langle 2, 4, 7\rangle$, which corresponds to the decreasing subsequence $\langle 5, 3, 1\rangle$ of $S$.
%
%Assume the decreasing maximum indices $S^\downarrow_{i, j}$ is given with respect to some interval $S_{i, j} = \langle s_i, s_{i+1}, ..., s_{j-1}\rangle$ in $S$, and consider the corresponding indices $S^\downarrow_{i, j+1}$ with respect to $S_{i, j+1} = \langle s_i, s_{i+1}, ..., s_{j}\rangle$. It is simple to assert that $S^\downarrow_{i, j+1}$ can be obtained by removing from the suffix of $S^\downarrow_{i, j}$ all indices $k$ such that $s_k \leq s_j$, and adding the index $j$ at the end of the resulting sequence. To compute $S^\downarrow_{i+1, j}$, one just remove the index $i$ from $S^\downarrow_{i, j}$ in case it is the first index in this series, and otherwise it is identical to $S^\downarrow_{i, j}$. In both cases, the time required for updating the data structure is proportional to the total number of element insertions and deletions from the index sequence.
%
%
%
%
%********************************
%
%
%In what follow, we describe a recursive computation for $H(P_j)$ for every prefix $P_j$ of $P$ (and in particular for $P = P_n$). Clearly,
%
%\begin{equation}\label{eq:H}
%H(P_j) = \min\left(H^r(P_j), H^l(P_j)\right).
%\end{equation}
%
%
%Consider an orientation $\dir[P_j] \in \mathcal{O}^r(P_j)$. By definition, there exists some $0 \leq i < j$ such that $\dir[P_j] = \dir[P'] \dir[P^r]_{i, j}$, where $\dir[P'] \in \mathcal{O}^l(P_i)$. Observe that any path in $\dir[P_j]$ cannot contain edges from both $\dir[P']$ (which is either empty when $i=0$, or otherwise ending with a leftward edge $(v_i, v_{i-1})$) and $\dir[P^r]_{i, j}$ (starting with a rightward edge $(v_i, v_{i+1})$). In particular, a heaviest path in $\dir[P_j]$ is fully contained in either $\dir[P']$ or $\dir[P^r]_{i, j}$. Therefore, $h(\dir[P_j]) = \max\left(h(\dir[P']), h(\dir[P^r]_{i, j})\right)$. An orientation that would minimize the heaviest path weight among all orientations of this form with the same $i$ value would thus have the weight $\max\left(H^l(P_i), h(\dir[P^r]_{i, j})\right)$, and we get that 
%
%\begin{equation}\label{eq:Hr}
%H^r(P_j) = \left\{\begin{array}{ll}
%0, & j = 0\\
%\min_{0 \leq i < j}\left(\max\left(H^l(P_i), h(\dir[P^r]_{i, j})\right)\right), & \text{otherwise}\\
%\end{array}\right.
%\end{equation}
%
%Symmetrically: 
%
%\begin{equation}\label{eq:Hl}
%H^l(P_j) = \left\{\begin{array}{ll}
%0, & j = 0\\
%\min_{0 \leq i < j}\left(\max\left(H^r(P_i), h(\dir[P^l]_{i, j})\right)\right), & \text{otherwise}\\
%\end{array}\right.
%\end{equation}
%
%
%Next, we describe how to efficiently compute the above recursion. 
%
%
%\begin{observation}
%	\label{obs:subpath}
%	For every $0 \leq i \leq j \leq n$, $H(P_{i, j}) \leq H(P)$, $h(P^r_{i, j}) \leq h(P^r)$, and $h(P^l_{i, j}) \leq h(P^l)$.
%\end{observation}
%
%The correctness of the observation follows from the fact that any orientation for $P$ induces an orientation for $P_{i, j}$ in which the weight of a heaviest path can only decrease. Next, consider equation~\ref{eq:Hr}, and denote by $k_P$ the maximal integer that minimizes the equation term for $n > 0$. 
%
%\begin{claim}
%	\label{clm:kp}
%	$H(P_{0, k_P}) < H(P_{0, k_P+1})$.
%\end{claim}
%
%\begin{proof}
%	By definition of $K_P$, $\max\left(H^l(P_{0, k_P}), h(P^r_{k_P, n})\right) < \max\left(H^l(P_{0, k_P + 1}), h(P^r_{k_P + 1, n})\right)$. Since $h(P^r_{k_P, n}) \geq h(P^r_{k_P+1, n})$ (from Obs.~\ref{obs:subpath}), it must be that $H(P_{0, k_P}) < H(P_{0, k_P+1})$
%\end{proof}
%
%
%\begin{claim}
%	\label{clm:kp_monotonicity}
%	For every $0 \leq j < n$, $K_{P_{0, j}} \leq K_P$.
%\end{claim}
%
%\begin{proof}
%	
%\end{proof}
