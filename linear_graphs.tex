
\section{An Algorithm for Linear Graphs}

%A \emph{linear graph} $P$ of length $n$ is an undirected graph with $n+1$
%vertices $V = \{v_0, v_1, \ldots, v_n\}$ and $n$ edges $E = \left\{\{v_i, v_{i+1}\} : 0\leq i < n\right\}$. 
Given a bi-weighted linear graph $P$ of length $n$ we assume that its vertices
are numbered from \textit{0} to \textit{n}, and denote the weights of
edges $(i,i+1)$ and  $(i+1,i)$ by $w(i,i+1)$ and $w(i+1,1)$, respectively.

We describe next an algorithm for finding an optimal orientation 
of a linear graph. The high level version of the algorithm makes no use of the details
of the cost function, be it $h_m$ or $h_s$. The description will therefore omit these subscripts. However, upon implementing the high level version, the difference between 
the two cost functions leads to a surprising difference in running times: it is linear 
under $h_s$ but quadratic under $h_m$.
\bigskip

{\bf Notation}:
\begin{itemize}
 	\item $P_{i, j}$ is the sub-graph of $P$ induced by the vertices $i,  \ldots, j$, and $P=P_{0,n}$. 
 	\item $\vec{P_{i, j}}$ denotes the oriented version 
 	of $P_{i, j}$ in which all edges are directed to the right (i.e. of the form $(i, {i+1})$),
 	and similarly  $\cev{P_{i, j}}$ denotes the oriented version 
 	of $P_{i, j}$ in which all edges are directed to the left.
 	\item $H^r[i]$ is the value of an optimal orientaton of $P_{0, i}$ under the constraint
 	that edge $\{i,i+1\}$ is directed towards $i+1$, and $H^{\ell}[i]$ is the value of an optimal orientaton of $P_{0, i}$ under the constraint
 	that edge $\{i,i+1\}$ is directed towards $i$.
\end{itemize}

The algorithm uses dynamic programming, and its basic step is to locate the last 
breakpoint of an optimal solution, in the sense that its two edges are either both
ingoing or both outgoing. 
\bigskip

\noindent \textbf{Algorithm BestCostPath ($P$)}:
\begin{enumerate}
	\item $H^r[0]=H^{\ell}[0]=0$;
	\item for $j=1$ to $n$ do
	\begin{enumerate}
	  \item \label{i.1} $H^{r}[j]=\min_{0\leq i <j} \max \{ H^{\ell}[i], h(\vec{P_{i, j}}) \}$;
	  \item \label{i.2} $H^{\ell}[j]=\min_{0\leq i <j} \max \{ H^r[i], h(\cev{P_{i, j}}) \}$;
	\end{enumerate}
	\item return $\max \{H^{r}[n],\ H^{\ell}[n]\}$.	
\end{enumerate}

Turning now to the running time analysis, we note first of all that the values 
$ h(\vec{P_{i, j}}), h(\cev{P_{i, j}})$ used in statements (\ref{i.1}) and (\ref{i.2})
can be computed in constant time if the cost function is $h_m$. Namely, if the $2n$ values 
$ h_m(\vec{P_{0, j}})$, $h_m(\cev{P_{0, j}})$, $1\leq j \leq n$ are precomputed in $O(n)$ time,
then
$h_m(\vec{P_{i, j}})=h_m(\vec{P_{0, j}})-h_m(\vec{P_{0, i}})
%=\sum_{k=i}^{j-1} w(k,k+1).$
$.

To prove that this is true also for the cost function $h_s$ takes more doing.

%It remains to determine the time needed for the computation of the minimum values 
%in those statements.
%On the face of it, it takes $O(n)$ time to compute one such value.
%We show now that the totality of these computations takes $O(n)$ time
%if the cost function is $h_s$.
%
%\begin{theorem}
%When the cost function used in Algorithm BestCostPath ($P$) is $h_s$ its running time is $O(n)$.
%\end{theorem}
%
%\noindent {\bf Proof}.
%According to the foregoing analysis we need to show that the total time needed for the computations of all minimum values in statements (\ref{i.1}) and (\ref{i.2}) is $O(n)$.
%The key to the proof is the following fact.
%\begin{claim}
%	Denote 
%	$$a^{\ell}[j]=\argmin_{0\leq i <j} \max \{ H_s^{\ell}[i], h_s(\vec{P_{i, j}}) \},
%   a^{r}[j]=\argmin_{0\leq i <j} \max \{ H_s^{r}[i], h_s(\vec{P_{i, j}}) \}.
%	$$
%	Then $a^{\ell}[j]\leq a^{\ell}[j+1]$, and $a^{r}[j]\leq a^{r}[j+1]$, $0\leq j <n$.
%\end{claim}
%
%\noindent The proof of the Claim rests on two observations:
%\begin{enumerate}
%	\item $h_s(\vec{P_{i, j}}$ is non-increasing as a function of $i$ and non-decreasing as a function of $j$. 
%\end{enumerate}
%\qed
%
%-----------------------------------------------------------------------------------------
%
%Let $P$ be a bi-weighted linear graph of length $n$. Denote:
%\begin{itemize}
%	\item $P_{i, j}$: the sub-graph of $P$ induced by the vertices $\left\{v_i, v_{i+1}, \ldots, v_j \right\}$. For short, denote by $P_j$ the prefix $P_{0, j}$ of $P$;
%	\item $\vec{P'} \vec{P''}$: an orientation of $P$ obtained by concatenating the orientations $\dir[P']$ and $\dir[P'']$ for some prefix $P_{k}$ and suffix $P_{k, n}$ of $P$, respectively;
%	\item $\dir[P^r]$, $\dir[P^l]$: the orientations of $P$ in which all edges are directed "rightward" (i.e. of the form $(v_i, v_{i+1})$) or "leftwards" (i.e. of the form $(v_{i+1}, v_i)$), respectively;
%	\item $\mathcal{O}^r(P) = \left\{\dir[P'] \dir[P^r]_{n-1,n} : \dir[P'] \in \mathcal{O}(P_{n-1})\right\}$ (i.e. the subset of $\mathcal{O}(P)$ containing all orientations of $P$ ending with the directed edge $(v_{n-1}, v_n)$);
%	\item $\mathcal{O}^l(P) = \mathcal{O}(P) \setminus \mathcal{O}^r(P) = \left\{\dir[P'] \dir[P^l]_{n-1,n} : \dir[P'] \in \mathcal{O}(P_{n-1})\right\}$ (i.e. the subset of $\mathcal{O}(P)$ containing all orientations of $P$ ending with the directed edge $(v_n, v_{n-1})$);
%	\item $H^r(P) = \min_{\dir[P] \in \mathcal{O}^r(P)} h(\dir[P])$;
%	\item $H^l(P) = \min_{\dir[P] \in \mathcal{O}^l(P)} h(\dir[P])$.
%\end{itemize}
%
%Before describing the computation of $H(P)$, we first describe an auxiliary computation of values of the form $h(\dir[P^r]_{i, j})$ which will be used within the computation of $H(P)$. This computation utilizes the \emph{deque} data structure to store information about $\dir[P^r]_{i, j}$ that will allow the efficient computation of $h(\dir[P^r]_{i, j})$. In addition, given the structure for the segment $\dir[P^r]_{i, j}$, the updated structure with respect to segments $\dir[P^r]_{i+1, j}$ and $\dir[P^r]_{i, j+1}$ can be computed efficiently. The computation of values of the form $h(\dir[P^l]_{i, j})$ is symmetric and we omit its explicit description. 
%
%Let $S = \langle s_0, s_1, ..., s_{m-1}\rangle$ be a series of numbers. When $m > 0$, denote by $k_S$ the index $0 \leq k < m$ so that $s_{k_S}$ is the maximum of $S$, and $k_S$ is the biggest such index if there are multiple maximal values in $S$. In other words, for every $k_S < k < m$, $s_{k_S} > s_k$. The \emph{decreasing maximum indices} $S^{\downarrow}$ of $S$ is inductively defined as follows. When $S$ is empty then $S^{\downarrow}$ is empty, and otherwise $S^{\downarrow}$ is the series of indices starting with $k_S$ and continuing with the decreasing maximum indices of the suffix $\langle s_{k_S+1}, s_{k_S+2}, ..., s_{m-1}\rangle$ of $S$. For example, for $S = \langle 5, 4, 5, 2, 3, -1, 0, 1\rangle$, $S^\downarrow = \langle 2, 4, 7\rangle$, which corresponds to the decreasing subsequence $\langle 5, 3, 1\rangle$ of $S$.
%
%Assume the decreasing maximum indices $S^\downarrow_{i, j}$ is given with respect to some interval $S_{i, j} = \langle s_i, s_{i+1}, ..., s_{j-1}\rangle$ in $S$, and consider the corresponding indices $S^\downarrow_{i, j+1}$ with respect to $S_{i, j+1} = \langle s_i, s_{i+1}, ..., s_{j}\rangle$. It is simple to assert that $S^\downarrow_{i, j+1}$ can be obtained by removing from the suffix of $S^\downarrow_{i, j}$ all indices $k$ such that $s_k \leq s_j$, and adding the index $j$ at the end of the resulting sequence. To compute $S^\downarrow_{i+1, j}$, one just remove the index $i$ from $S^\downarrow_{i, j}$ in case it is the first index in this series, and otherwise it is identical to $S^\downarrow_{i, j}$. In both cases, the time required for updating the data structure is proportional to the total number of element insertions and deletions from the index sequence.
%
%
%
%
%********************************
%
%
%In what follow, we describe a recursive computation for $H(P_j)$ for every prefix $P_j$ of $P$ (and in particular for $P = P_n$). Clearly,
%
%\begin{equation}\label{eq:H}
%H(P_j) = \min\left(H^r(P_j), H^l(P_j)\right).
%\end{equation}
%
%
%Consider an orientation $\dir[P_j] \in \mathcal{O}^r(P_j)$. By definition, there exists some $0 \leq i < j$ such that $\dir[P_j] = \dir[P'] \dir[P^r]_{i, j}$, where $\dir[P'] \in \mathcal{O}^l(P_i)$. Observe that any path in $\dir[P_j]$ cannot contain edges from both $\dir[P']$ (which is either empty when $i=0$, or otherwise ending with a leftward edge $(v_i, v_{i-1})$) and $\dir[P^r]_{i, j}$ (starting with a rightward edge $(v_i, v_{i+1})$). In particular, a heaviest path in $\dir[P_j]$ is fully contained in either $\dir[P']$ or $\dir[P^r]_{i, j}$. Therefore, $h(\dir[P_j]) = \max\left(h(\dir[P']), h(\dir[P^r]_{i, j})\right)$. An orientation that would minimize the heaviest path weight among all orientations of this form with the same $i$ value would thus have the weight $\max\left(H^l(P_i), h(\dir[P^r]_{i, j})\right)$, and we get that 
%
%\begin{equation}\label{eq:Hr}
%H^r(P_j) = \left\{\begin{array}{ll}
%0, & j = 0\\
%\min_{0 \leq i < j}\left(\max\left(H^l(P_i), h(\dir[P^r]_{i, j})\right)\right), & \text{otherwise}\\
%\end{array}\right.
%\end{equation}
%
%Symmetrically: 
%
%\begin{equation}\label{eq:Hl}
%H^l(P_j) = \left\{\begin{array}{ll}
%0, & j = 0\\
%\min_{0 \leq i < j}\left(\max\left(H^r(P_i), h(\dir[P^l]_{i, j})\right)\right), & \text{otherwise}\\
%\end{array}\right.
%\end{equation}
%
%
%Next, we describe how to efficiently compute the above recursion. 
%
%
%\begin{observation}
%	\label{obs:subpath}
%	For every $0 \leq i \leq j \leq n$, $H(P_{i, j}) \leq H(P)$, $h(P^r_{i, j}) \leq h(P^r)$, and $h(P^l_{i, j}) \leq h(P^l)$.
%\end{observation}
%
%The correctness of the observation follows from the fact that any orientation for $P$ induces an orientation for $P_{i, j}$ in which the weight of a heaviest path can only decrease. Next, consider equation~\ref{eq:Hr}, and denote by $k_P$ the maximal integer that minimizes the equation term for $n > 0$. 
%
%\begin{claim}
%	\label{clm:kp}
%	$H(P_{0, k_P}) < H(P_{0, k_P+1})$.
%\end{claim}
%
%\begin{proof}
%	By definition of $K_P$, $\max\left(H^l(P_{0, k_P}), h(P^r_{k_P, n})\right) < \max\left(H^l(P_{0, k_P + 1}), h(P^r_{k_P + 1, n})\right)$. Since $h(P^r_{k_P, n}) \geq h(P^r_{k_P+1, n})$ (from Obs.~\ref{obs:subpath}), it must be that $H(P_{0, k_P}) < H(P_{0, k_P+1})$
%\end{proof}
%
%
%\begin{claim}
%	\label{clm:kp_monotonicity}
%	For every $0 \leq j < n$, $K_{P_{0, j}} \leq K_P$.
%\end{claim}
%
%\begin{proof}
%	
%\end{proof}
